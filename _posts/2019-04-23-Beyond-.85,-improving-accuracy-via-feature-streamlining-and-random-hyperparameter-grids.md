---
layout: post
title: "Beyond .85, improving accuracy via feature streamlining and random hyperparameter grids"
date: 2019-04-23
---

A random forests algorithm applied to Lending Club data predicted default rates with .85 accuracy out of the box. Could streamlining the feature set from an initial 40, and randomly sampling grids of hyperparameter values, maintain or improve that accuracy?

This is an extensive example of my work on a data science project, soup to nuts. To see what I can do, and see the answer to the question above, click through the links below.

NBViewer supports handy Jupyter notebook internal links for easy navigation:
https://nbviewer.jupyter.org/github/RossBrownPhD/Work_Samples_and_Resume/tree/master/Beyond_.85_Improving_accuracy_via_feature_streamlining_and_random_grids_of_hyperparameters.ipynb

Github has a less interactive version:
https://github.com/RossBrownPhD/Work_Samples_and_Resume/blob/master/Beyond_.85_Improving_accuracy_via_feature_streamlining_and_random_grids_of_hyperparameters.ipynb
