---
layout: post
title: "Beyond .85, improving accuracy via feature streamlining & random hyperparameter grids"
date: 2019-04-23
---

Beyond .85: Improving accuracy via feature set streamlining & random hyperparameter grids 

A random forests algorithm applied to Lending Club data predicted default rates with .85 accuracy out of the box. Could streamlining the feature set from an initial 40, and randomly sampling grids of hyperparameter values, maintain or improve that accuracy? 

This is an extensive example of my work on a data science project, soup to nuts. Copy and paste a link below to see what I can do, and see the answer to the question above:

NBViewer Link (handy Jupyter notebook internal links will work here):

https://nbviewer.jupyter.org/github/RossBrownPhD/Work_Samples_and_Resume/tree/master/10_Beyond_.85,_improving_accuracy_via_feature_streamlining_&_random_grids_of_hyperparameters.ipynb
 
Less interactive version on github:

https://github.com/RossBrownPhD/Work_Samples_and_Resume/blob/master/10_Beyond_.85,_improving_accuracy_via_feature_streamlining_&_random_grids_of_hyperparameters.ipynb
